otjowjvejwofqmldklvkmq;o;qovmdalkfql kdmlqpvemoqmemlvkfmlkwrjpojwivlkqml
 The CNN model designed for a classification task consists of two convolutional layers with 32 and 64 filters each, using a kernel size of 3 and stride of 1, which are essential for feature extraction from the input images. Batch normalization follows each convolutional layer and the first fully connected layer to help in stabilizing the learning process. Dropout layers with rates of 0.3 and 0.5 are integrated to prevent overfitting by randomly disabling a fraction of neurons during training. Max pooling with a 2x2 window size is employed after each convolutional layer to increase receptive field size. The network includes two fully connected layers to classifiers, with the first having 128 output features and the second corresponding to 11 output classes. Activation functions are ReLU used after each convolutional and the first fully connected layer for introducing non-linearity and preventing vanishing gradient problems. LogSoftmax applied at the final layer for probability distribution across the classes. The choice of Cross-Entropy Loss as the loss function aligns with the classification nature of the task. Lastly, the Adam optimizer with a learning rate of 0.001 is chosen for its effectiveness in quickly converging.